"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
import torchvision.models.optical_flow.raft as raft
from typing import Callable, Iterable, List, Optional, Tuple
from torch import Tensor
from torchvision.models._api import WeightsEnum, register_model
from torchvision.models._utils import handle_legacy_interface

all = ...
class ConvexMaskPredictor(nn.Module):
    def __init__(self, *, in_channels: int, hidden_size: int, upsample_factor: int, multiplier: float = ...) -> None:
        ...
    
    def forward(self, x: Tensor) -> Tensor:
        ...
    


def get_correlation(left_feature: Tensor, right_feature: Tensor, window_size: Tuple[int, int] = ..., dilate: Tuple[int, int] = ...) -> Tensor:
    """Function that computes a correlation product between the left and right features.

    The correlation is computed in a sliding window fashion, namely the left features are fixed
    and for each ``(i, j)`` location we compute the correlation with a sliding window anchored in
    ``(i, j)`` from the right feature map. The sliding window selects pixels obtained in the range of the sliding
    window; i.e ``(i - window_size // 2, i + window_size // 2)`` respectively ``(j - window_size // 2, j + window_size // 2)``.
    """
    ...

class IterativeCorrelationLayer(nn.Module):
    def __init__(self, groups: int = ..., search_window_1d: Tuple[int, int] = ..., search_dilate_1d: Tuple[int, int] = ..., search_window_2d: Tuple[int, int] = ..., search_dilate_2d: Tuple[int, int] = ...) -> None:
        ...
    
    def forward(self, left_feature: Tensor, right_feature: Tensor, flow: Tensor, window_type: str = ...) -> Tensor:
        """Function that computes 1 pass of non-offsetted Group-Wise correlation"""
        ...
    


class AttentionOffsetCorrelationLayer(nn.Module):
    def __init__(self, groups: int = ..., attention_module: Optional[nn.Module] = ..., search_window_1d: Tuple[int, int] = ..., search_dilate_1d: Tuple[int, int] = ..., search_window_2d: Tuple[int, int] = ..., search_dilate_2d: Tuple[int, int] = ...) -> None:
        ...
    
    def forward(self, left_feature: Tensor, right_feature: Tensor, flow: Tensor, extra_offset: Tensor, window_type: str = ...) -> Tensor:
        """Function that computes 1 pass of offsetted Group-Wise correlation

        If the class was provided with an attention layer, the left and right feature maps
        will be passed through a transformer first
        """
        ...
    


class AdaptiveGroupCorrelationLayer(nn.Module):
    """
    Container for computing various correlation types between a left and right feature map.
    This module does not contain any optimisable parameters, it's solely a collection of ops.
    We wrap in a nn.Module for torch.jit.script compatibility

    Adaptive Group Correlation operations from: https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Practical_Stereo_Matching_via_Cascaded_Recurrent_Network_With_Adaptive_Correlation_CVPR_2022_paper.pdf

    Canonical reference implementation: https://github.com/megvii-research/CREStereo/blob/master/nets/corr.py
    """
    def __init__(self, iterative_correlation_layer: IterativeCorrelationLayer, attention_offset_correlation_layer: AttentionOffsetCorrelationLayer) -> None:
        ...
    
    def forward(self, left_features: Tensor, right_features: Tensor, flow: torch.Tensor, extra_offset: Optional[Tensor], window_type: str = ..., iter_mode: bool = ...) -> Tensor:
        ...
    


def elu_feature_map(x: Tensor) -> Tensor:
    """Elu feature map operation from: https://arxiv.org/pdf/2006.16236.pdf"""
    ...

class LinearAttention(nn.Module):
    """
    Linear attention operation from: https://arxiv.org/pdf/2006.16236.pdf
    Canonical implementation reference: https://github.com/idiap/fast-transformers/blob/master/fast_transformers/attention/linear_attention.py
    LoFTR implementation reference: https://github.com/zju3dv/LoFTR/blob/2122156015b61fbb650e28b58a958e4d632b1058/src/loftr/loftr_module/linear_attention.py
    """
    def __init__(self, eps: float = ..., feature_map_fn: Callable[[Tensor], Tensor] = ...) -> None:
        ...
    
    def forward(self, queries: Tensor, keys: Tensor, values: Tensor, q_mask: Optional[Tensor] = ..., kv_mask: Optional[Tensor] = ...) -> Tensor:
        """
        Args:
            queries (torch.Tensor): [N, S1, H, D]
            keys (torch.Tensor): [N, S2, H, D]
            values (torch.Tensor): [N, S2, H, D]
            q_mask (torch.Tensor): [N, S1] (optional)
            kv_mask (torch.Tensor): [N, S2] (optional)
        Returns:
            queried_values (torch.Tensor): [N, S1, H, D]
        """
        ...
    


class SoftmaxAttention(nn.Module):
    """
    A simple softmax attention  operation
    LoFTR implementation reference: https://github.com/zju3dv/LoFTR/blob/2122156015b61fbb650e28b58a958e4d632b1058/src/loftr/loftr_module/linear_attention.py
    """
    def __init__(self, dropout: float = ...) -> None:
        ...
    
    def forward(self, queries: Tensor, keys: Tensor, values: Tensor, q_mask: Optional[Tensor] = ..., kv_mask: Optional[Tensor] = ...) -> Tensor:
        """
        Computes classical softmax full-attention between all queries and keys.

        Args:
            queries (torch.Tensor): [N, S1, H, D]
            keys (torch.Tensor): [N, S2, H, D]
            values (torch.Tensor): [N, S2, H, D]
            q_mask (torch.Tensor): [N, S1] (optional)
            kv_mask (torch.Tensor): [N, S2] (optional)
        Returns:
            queried_values: [N, S1, H, D]
        """
        ...
    


class PositionalEncodingSine(nn.Module):
    """
    Sinusoidal positional encodings

    Using the scaling term from https://github.com/megvii-research/CREStereo/blob/master/nets/attention/position_encoding.py
    Reference implementation from https://github.com/facebookresearch/detr/blob/8a144f83a287f4d3fece4acdf073f387c5af387d/models/position_encoding.py#L28-L48
    """
    def __init__(self, dim_model: int, max_size: int = ...) -> None:
        ...
    
    def forward(self, x: Tensor) -> Tensor:
        """
        Args:
            x: [B, C, H, W]

        Returns:
            x: [B, C, H, W]
        """
        ...
    


class LocalFeatureEncoderLayer(nn.Module):
    """
    LoFTR transformer module from: https://arxiv.org/pdf/2104.00680.pdf
    Canonical implementations at: https://github.com/zju3dv/LoFTR/blob/master/src/loftr/loftr_module/transformer.py
    """
    def __init__(self, *, dim_model: int, num_heads: int, attention_module: Callable[..., nn.Module] = ...) -> None:
        ...
    
    def forward(self, x: Tensor, source: Tensor, x_mask: Optional[Tensor] = ..., source_mask: Optional[Tensor] = ...) -> Tensor:
        """
        Args:
            x (torch.Tensor): [B, S1, D]
            source (torch.Tensor): [B, S2, D]
            x_mask (torch.Tensor): [B, S1] (optional)
            source_mask (torch.Tensor): [B, S2] (optional)
        """
        ...
    


class LocalFeatureTransformer(nn.Module):
    """
    LoFTR transformer module from: https://arxiv.org/pdf/2104.00680.pdf
    Canonical implementations at: https://github.com/zju3dv/LoFTR/blob/master/src/loftr/loftr_module/transformer.py
    """
    def __init__(self, *, dim_model: int, num_heads: int, attention_directions: List[str], attention_module: Callable[..., nn.Module] = ...) -> None:
        ...
    
    def forward(self, left_features: Tensor, right_features: Tensor, left_mask: Optional[Tensor] = ..., right_mask: Optional[Tensor] = ...) -> Tuple[Tensor, Tensor]:
        """
        Args:
            left_features (torch.Tensor): [N, S1, D]
            right_features (torch.Tensor): [N, S2, D]
            left_mask (torch.Tensor): [N, S1] (optional)
            right_mask (torch.Tensor): [N, S2] (optional)
        Returns:
            left_features (torch.Tensor): [N, S1, D]
            right_features (torch.Tensor): [N, S2, D]
        """
        ...
    


class PyramidDownsample(nn.Module):
    """
    A simple wrapper that return and Avg Pool feature pyramid based on the provided scales.
    Implicitly returns the input as well.
    """
    def __init__(self, factors: Iterable[int]) -> None:
        ...
    
    def forward(self, x: torch.Tensor) -> List[Tensor]:
        ...
    


class CREStereo(nn.Module):
    """
    Implements CREStereo from the `"Practical Stereo Matching via Cascaded Recurrent Network
    With Adaptive Correlation" <https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Practical_Stereo_Matching_via_Cascaded_Recurrent_Network_With_Adaptive_Correlation_CVPR_2022_paper.pdf>`_ paper.
    Args:
        feature_encoder (raft.FeatureEncoder): Raft-like Feature Encoder module extract low-level features from inputs.
        update_block (raft.UpdateBlock): Raft-like Update Block which recursively refines a flow-map.
        flow_head (raft.FlowHead): Raft-like Flow Head which predics a flow-map from some inputs.
        self_attn_block (LocalFeatureTransformer): A Local Feature Transformer that performs self attention on the two feature maps.
        cross_attn_block (LocalFeatureTransformer): A Local Feature Transformer that performs cross attention between the two feature maps
            used in the Adaptive Group Correlation module.
        feature_downsample_rates (List[int]): The downsample rates used to build a feature pyramid from the outputs of the `feature_encoder`. Default: [2, 4]
        correlation_groups (int): In how many groups should the features be split when computer per-pixel correlation. Defaults 4.
        search_window_1d (Tuple[int, int]): The alternate search window size in the x and y directions for the 1D case. Defaults to (1, 9).
        search_dilate_1d (Tuple[int, int]): The dilation used in the `search_window_1d` when selecting pixels. Similar to `nn.Conv2d` dilate. Defaults to (1, 1).
        search_window_2d (Tuple[int, int]): The alternate search window size in the x and y directions for the 2D case. Defaults to (3, 3).
        search_dilate_2d (Tuple[int, int]): The dilation used in the `search_window_2d` when selecting pixels. Similar to `nn.Conv2d` dilate. Defaults to (1, 1).
    """
    def __init__(self, *, feature_encoder: raft.FeatureEncoder, update_block: raft.UpdateBlock, flow_head: raft.FlowHead, self_attn_block: LocalFeatureTransformer, cross_attn_block: LocalFeatureTransformer, feature_downsample_rates: Tuple[int, ...] = ..., correlation_groups: int = ..., search_window_1d: Tuple[int, int] = ..., search_dilate_1d: Tuple[int, int] = ..., search_window_2d: Tuple[int, int] = ..., search_dilate_2d: Tuple[int, int] = ...) -> None:
        ...
    
    def forward(self, left_image: Tensor, right_image: Tensor, flow_init: Optional[Tensor] = ..., num_iters: int = ...) -> List[Tensor]:
        ...
    


_COMMON_META = ...
class CREStereo_Base_Weights(WeightsEnum):
    """The metrics reported here are as follows.

    ``mae`` is the "mean-average-error" and indicates how far (in pixels) the
    predicted disparity is from its true value (equivalent to ``epe``). This is averaged over all pixels
    of all images. ``1px``, ``3px``, ``5px`` and indicate the percentage of pixels that have a lower
    error than that of the ground truth. ``relepe`` is the "relative-end-point-error" and is the
    average ``epe`` divided by the average ground truth disparity. ``fl-all`` corresponds to the average of pixels whose epe
    is either <3px, or whom's ``relepe`` is lower than 0.05 (therefore higher is better).

    """
    MEGVII_V1 = ...
    CRESTEREO_ETH_MBL_V1 = ...
    CRESTEREO_FINETUNE_MULTI_V1 = ...
    DEFAULT = ...


@register_model()
@handle_legacy_interface(weights=("pretrained", CREStereo_Base_Weights.MEGVII_V1))
def crestereo_base(*, weights: Optional[CREStereo_Base_Weights] = ..., progress=..., **kwargs) -> CREStereo:
    """CREStereo model from
    `Practical Stereo Matching via Cascaded Recurrent Network
    With Adaptive Correlation <https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Practical_Stereo_Matching_via_Cascaded_Recurrent_Network_With_Adaptive_Correlation_CVPR_2022_paper.pdf>`_.

    Please see the example below for a tutorial on how to use this model.

    Args:
        weights(:class:`~torchvision.prototype.models.depth.stereo.CREStereo_Base_Weights`, optional): The
            pretrained weights to use. See
            :class:`~torchvision.prototype.models.depth.stereo.CREStereo_Base_Weights`
            below for more details, and possible values. By default, no
            pre-trained weights are used.
        progress (bool): If True, displays a progress bar of the download to stderr. Default is True.
        **kwargs: parameters passed to the ``torchvision.prototype.models.depth.stereo.raft_stereo.RaftStereo``
            base class. Please refer to the `source code
            <https://github.com/pytorch/vision/blob/main/torchvision/models/optical_flow/crestereo.py>`_
            for more details about this class.

    .. autoclass:: torchvision.prototype.models.depth.stereo.CREStereo_Base_Weights
        :members:
    """
    ...

